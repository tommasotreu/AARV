We reviewed gravitational time delays as a tool for measuring
cosmological parameters. In addition to giving a brief introduction to
theoretical underpinnings of the method, we discussed the past history
of the field, before turning to present day accomplishments and the
challenges ahead. The main points of this review can be summarized as
follows:

\begin{enumerate}
\item From a theoretical point of view gravitational time delays are a clean and well understood probe of cosmic acceleration. Conceptually each time delay measurement provides a direct one step measurement of absolute distance. The typical redshifts of deflectors and sources span the range between $z\sim2$ and today, covering the cosmic time when dark energy arises to prominence.
\item Even though the potential cosmological application of strongly lensed time variable sources was recognized as early as 1964, it took decades for the method to come to fruition in practice. Two sets of challenges were overcome during the past 15 years. Observationally, the main challenge was organizing long term monitoring campaigns and mustering the range of resources required to constrain accurate mass models. Theoretically, the main challenge consisted in learning how to exploit the available information to construct lens models with realistic estimate of the uncertainties.
\item Currently, it has been demonstrated through blind measurements that each individual system can deliver a measurement of absolute distance to about 6-7\%total uncertainty.  The power of the method is currently limited by the number of systems with well measured time delays and sufficient ancillary data to carry out detailed modeling ($\lesssim10$ at the time of this writing).
\item Systematic searches for strongly lensed quasars are under way and should be able to increase by 1-2 orders of magnitude in the next decade the numbers of known lenses suitable for time delay cosmography.
\item As large samples of lensed quasars become available, different strategies will need to be implemented to extract the most cosmological information from them. Before LSST, dedicated monitoring campaigns will the required to measure each time delay. Under these circumstances, a viable strategy will consist in obtain high quality follow-up data in order to minimize the uncertainty per system.. These include: high resolution imaging from space or with adaptive optics; redshifts, stellar velocity dispersions, and spatially resolved kinematics of the deflectors from the James Webb Space Telescope or large and extremely large ground based telescopes.
\item LSST can potentially alter the landscape if it can deliver hundreds of time dlays from the survey data themselves. If this is the case, then it is possible and perhaps desirable that a mixed strategy where large numbers of systems with relatively scant follow-up data can be analyzed with hierarchical models based on priors based on the in depth analysis of smaller precursor datasets.
\end{enumerate}

\textbf{TT: The above may be less concise than we want it to be. Free free to have a go at condensing it.}

In conclusion, it would be presumptuos to forecast how and when we are
going to understand the deep mystery of cosmic acceleration, but it
would be foolish not to try by means of a theoretically sound and
cost-effective tool like gravitational time delays. Studying lensed
quasars will allow us, in the worst-case scenario, to learn much about
the universe\footnote{Lensed quasars have been used to study topics ranging from the nature of dark matter
\citep{Metcalf:2005p1203,Xu++09,Veg++14,Nie++14} to the physics of
black hole acretion disks \citep{PMK08,Blackburne:2010p6600}, to the
initial mass function of stars \citep{SCH++15}}. If nature is kind
enough to provide us with the best-case scenario, we will discover
something about the fundamental nature of space time.

