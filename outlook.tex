%Motivation.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

% \subsection{Cosmic complementarity [TT]}
% PJM: I think this could function as the preamble for this section.
% We already have some mention of other probes in earlier sections,
% so it makes sense to motivate the forecasts in the next two subsections
% with the context being joint analysis. What do you think?

In this section we discuss the future of time delay cosmography and
present an ambitious yet realistic roadmap of how to improve the
measurement in the next decade. In order to construct the roadmap, we
will discuss in detail how to decrease the random uncertainties
(i.e. precision; Section~\ref{ssec:precision}), and what systematic
uncertainties will need to be controlled as the random uncertainties
decrease (i.e. accuracy; Section~\ref{ssec:accuracy}).

Before we lay out the roadmap, however, it is good to pause and
reflect on the broader context, and ask whether this is a worthy
endeavour.  Understanding the nature of dark energy is one of the most
profound questions in all of physics, and it is thus not surprising
that the efforts of many scientists and funding agencies have been
directed towards this goal. Dedicated instruments, telescopes, and
satellites are being built or planned with budgets that range from the
tens of millions of dollars into the billions (Euclid, WFIRST). Given
the steep challenges associated with each technique, most scientist
agree that is important to pursue several independent ones at the same
time (DETF, DESC). First, we do not know which techniques will live up
to their promise and which ones will be stymied by hitherto unknown
systematic effects, when we reduce the random uncertainties by one or
more orders of magnitudes. Second, as discussed in the introduction,
extraordinary claims require extraordinay proof, so it will certainly
require more than one independent measurement to convince the
community that, for example, dark energy is not the cosmological
constant ($w\neq-1$).

Given this context, deciding whether time delay cosmography is worth
pursuing boils down to three simpler questions. The first question is
whether time delays contain valuable information {\it independent} of
other cosmological probes. As detailed in Section~\ref{sec:cospars},
the answer is a resounding yes: gravitational time delays are
virtually independent of the uncertainties affecting the other
established probes of dark energy, and provide valuable complementary
information, chiefly on the Hubble constant, which is commonly
regarding as one of the essential ingredients for interpreting other
datasets such a the cosmic microwave background (Hu, Weinberg, Kim).
We will expand on this topic in the remainder of this section by
showing cosmological forecasts for gravitational time delays by
themselves and in combination with other probes.

The second question is whether it is feasible to achieve an {\it
interesting} level of precision and accuracy in coming years. In this
mindset, {\it interesting} is defined as having total uncertainties
comparable to that of other contemporary probes. This will be
discussed in detail below.

The third and final question is what is the {\it cost} of pursuing
this roadmap, and how does it compare to other probes. Our aim is not
to compute a full cost accounting, which will be almost impossible
considering that each probe involves facilities, observatories,
computing and brainpower, well beyond the boundaries of any individual
project, collaboration, or funding agency. Not to mention that the
marginal cost of adding a technique to an existing program or
facility, is very different from what the cost of building a facility
just for that purpose. For example, the cost of monitoring strongly
lensed quasars in LSST data is much less than building and operating
the LSST. Instead, we will aim to give an approximate sense of the
observational and human resources that will be needed to pursue the
roadmap. Having voted with our feet, we obviously think that the
relatively low {\it cost} and the relatively low risk of time delay
cosmology well justifies adding it to the investment portfolio of a
modern cosmologist. Hopefully the rest of this section will give the
reader enough information to make up their own mind on this matter.


%What's the point? Arent' other probes already doing it? Our place in the
%cosmology ecosystem. Refer back to current constraints from \citet{Suy++13} in
%Section~\ref{sec:cospars}, which show other probes.
%Discuss place relative to other distance indicators
%like Cepheids, BAO, Sne. Linder SL + SNe plots.
%Then complementarity with growth of structure
%probes like weak lensing, clusters etc etc. How important is H0?
%Weinberg et al 2013, Kim et al 2014. Figure 48 from Weinberg?

%Importance of multiple INDEPENDENT measurements for discovery of new
%physics, and to ensure overall accuracy of cosmological parameters.
%Define accuracy. Define precision. Segue. cost


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{Precision [PJM]}
\label{ssec:precision}

With considerable observational and data analysis effort, the
feasibility of reaching a precision of 6-7\% in time delay distance
per lens has been demonstrated. The contributions to this error budget
from the time delay measurement, mass model, and environment
correction are at present approximately equal and somewhat larger than
the estimated systematic errors. In this situation it makes sense to
enlarge the sample of lenses, in order to beat down the statistical
uncertainties. We return to the question of how to reduce the residual
systematic errors in the next section.

\citet{C+M09b} made initial Fisher matrix forecasts of the likely
available precision on $H_0$ in large future surveys. They considered
several possible samples, concluding that 100 well-measured systems
(with 5\% distance precision each) should provide sub-percent precision
on the Hubble constant, and provide  dark energy parameter constraints
that are competitive with optimistic forecasts of other ``Stage IV''
cosmological probes. They also note that comparable constraints could be
available from a sample of 4000 time delay lens systems, each with only
photometric redshifts and simple image configuration model constraints
\citep[following][]{Ogu07b}.  Continued investigation of both samples
seems warranted, keeping in mind that the size of such a photometric
sample would be set by the availability of time delays measured at the
$\pm 2$~day level.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!ht]
\centering\includegraphics[width=0.9\linewidth]{figures/Coe+Moustakas09_fig14.pdf}
\caption{Fisher matrix forecasts of cosmological parameters, based on
Dark Energy Task Force assumptions and having 5\% distance precision
for each of 100 time delay lenses. The Stage IV cosmological probes
being compared in an  open CDM cosmological model with time-variable
dark energy equation of state are weak lensing (WL), BAO, supernovae
(SN), cluster mass function (CL) and time delay cosmography (TD).
Figure reproduced from \citet{C+M09b}.}
\label{fig:fisher}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Extrapolations to N lenses assuming X\% precision per time delay
% distance, forecasts.

% FIGURE: Forecasts for 10,50,100,1000 lenses for various cosmological
% models (w, wa+w0, curvature etc etc). CosmoSIS forecasts (ackn. Dave \&
% Elise, ask them).

What will it take to reach 5\% distance precision in each of 100 lenses?
If \citet{LiaoEtal2015} are right, this sized sample of lenses with
precise time delays should be able to be constructed from analysis of
the LSST light curves. The lens models for these systems should be able
to be constrained to sufficient precision using high resolution imaging
from next generation AO facilities, giant segmented mirror telescopes,
JWST and even WFIRST \citep{Men++15}. High overall mass model precision
will require good spectroscopic data as well: integral field units on
these same telescopes should be able to provide this. The contribution
to the distance uncertainty from the line of sight was particularly high
in the  cases of B1608 and RXJ1131: in most other systems in a large
sample the effects of the environment should be lesser, and they should
(effectively) average down \citep{Col++13}. The main challenge in
reaching the required Stage IV is likely to be the analysis cost: lens
modeling is something of a craft, and scaling up to large samples could
pose problems. Distributing the work among a large team of modelers will
be needed, an approach that is proving effective in the Frontier Fields
cluster lensing project \citep{FF}.

Note on approximation of forecasting using just Ddt,
and not DA as well: above plot is conservative.
Cite Jee et al paper 1 for pointing this out,
show figure from Jee et al paper 2?


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{Accuracy [PJM]}
\label{ssec:accuracy}

Discussion of systematic uncertainties:

1) Time delay measurement. Light curve quality.

2) Lens mass modeling. Percent-level systematics due to model
assumptions (ie MSD). IFU observations, resolved stellar kinematics.
Ensembles.

3) Environment and line of sight

Other things: time delay perturbations (someone's noise is somebody else's signal..)
The importance of blinding.

[DO WE WANT TO HAVE SEPARATE SUBSECTION CALLED ROADMAP, OR IS IT GOING
TO BE EMBEDDED IN THE TWO SUBSECTIONS ABOVE. I THINK IT MAY BE
EFFECTIVE TO CALL IT OUT]
