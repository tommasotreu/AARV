%Motivation.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

% \subsection{Cosmic complementarity [TT]}
% PJM: I think this could function as the preamble for this section.
% We already have some mention of other probes in earlier sections,
% so it makes sense to motivate the forecasts in the next two subsections
% with the context being joint analysis. What do you think?

In this section we discuss the future of time delay cosmography and
present an ambitious yet realistic roadmap of how to improve the
measurement in the next decade. In order to construct the roadmap, we
will discuss in detail how to decrease the random uncertainties
(i.e. precision; Section~\ref{ssec:precision}), and what systematic
uncertainties will need to be controlled as the random uncertainties
decrease (i.e. accuracy; Section~\ref{ssec:accuracy}).

Before we lay out the roadmap, however, it is good to pause and
reflect on the broader context, and ask whether this is a worthy
endeavour.  Understanding the nature of dark energy is one of the most
profound questions in all of physics, and it is thus not surprising
that the efforts of many scientists and funding agencies have been
directed towards this goal. Dedicated instruments, telescopes, and
satellites are being built or planned with budgets that range from the
tens of millions of dollars into the billions (Euclid, WFIRST). Given
the steep challenges associated with each technique, most scientist
agree that is important to pursue several independent ones at the same
time (DETF, DESC). First, we do not know which techniques will live up
to their promise and which ones will be stymied by hitherto unknown
systematic effects, when we reduce the random uncertainties by one or
more orders of magnitudes. Second, as discussed in the introduction,
extraordinary claims require extraordinay proof, so it will certainly
require more than one independent measurement to convince the
community that, for example, dark energy is not the cosmological
constant ($w\neq-1$).

Given this context, deciding whether time delay cosmography is worth
pursuing boils down to three simpler questions. The first question is
whether time delays contain valuable information {\it independent} of
other cosmological probes. As detailed in Section~\ref{sec:cospars},
the answer is a resounding yes: gravitational time delays are
virtually independent of the uncertainties affecting the other
established probes of dark energy, and provide valuable complementary
information, chiefly on the Hubble constant, which is commonly
regarding as one of the essential ingredients for interpreting other
datasets such a the cosmic microwave background (Hu, Weinberg, Kim).
We will expand on this topic in the remainder of this section by
showing cosmological forecasts for gravitational time delays by
themselves and in combination with other probes.

The second question is whether it is feasible to achieve an {\it
interesting} level of precision and accuracy in coming years. In this
mindset, {\it interesting} is defined as having total uncertainties
comparable to that of other contemporary probes. This will be
discussed in detail below.

The third and final question is what is the {\it cost} of pursuing
this roadmap, and how does it compare to other probes. Our aim is not
to compute a full cost accounting, which will be almost impossible
considering that each probe involves facilities, observatories,
computing and brainpower, well beyond the boundaries of any individual
project, collaboration, or funding agency. Not to mention that the
marginal cost of adding a technique to an existing program or
facility, is very different from what the cost of building a facility
just for that purpose. For example, the cost of monitoring strongly
lensed quasars in LSST data is much less than building and operating
the LSST. Instead, we will aim to give an approximate sense of the
observational and human resources that will be needed to pursue the
roadmap. Having voted with our feet, we obviously think that the
relatively low {\it cost} and the relatively low risk of time delay
cosmology well justifies adding it to the investment portfolio of a
modern cosmologist. Hopefully the rest of this section will give the
reader enough information to make up their own mind on this matter.


%What's the point? Arent' other probes already doing it? Our place in the
%cosmology ecosystem. Refer back to current constraints from \citet{Suy++13} in
%Section~\ref{sec:cospars}, which show other probes.
%Discuss place relative to other distance indicators
%like Cepheids, BAO, Sne. Linder SL + SNe plots.
%Then complementarity with growth of structure
%probes like weak lensing, clusters etc etc. How important is H0?
%Weinberg et al 2013, Kim et al 2014. Figure 48 from Weinberg?

%Importance of multiple INDEPENDENT measurements for discovery of new
%physics, and to ensure overall accuracy of cosmological parameters.
%Define accuracy. Define precision. Segue. cost


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{Precision [PJM]}
\label{ssec:precision}

With considerable observational and data analysis effort, the
feasibility of reaching a {\it precision} of 6-7\% in time delay distance
per lens has been demonstrated. The contributions to this statistical
error budget
from the time delay measurement, mass model, and environment
correction are at present approximately equal, and somewhat larger than
the estimated systematic errors. In this situation it makes sense to
enlarge the sample of lenses, in order to beat down the statistical
uncertainties. We return to the question of how to reduce the residual
systematic errors in the next section.

\citet{C+M09b} made initial Fisher matrix forecasts of the likely
available precision on $H_0$ in large future surveys. They considered
several possible samples, concluding that 100 well-measured systems
(with 5\% distance precision each) should provide sub-percent precision
on the Hubble constant, and provide  dark energy parameter constraints
that are competitive with optimistic forecasts of other ``Stage IV''
cosmological probes. They also note that comparable constraints could be
available from a sample of 4000 time delay lens systems, each with only
photometric redshifts and simple image configuration model constraints
\citep[following][]{Ogu07b}.  Continued investigation of both samples
seems warranted, keeping in mind that the size of such a photometric
sample would be set by the availability of time delays measured at the
$\pm$~several day level.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!ht]
\centering\includegraphics[width=0.9\linewidth]{figures/Coe+Moustakas09_fig14.pdf}
\caption{Fisher matrix forecasts of cosmological parameters, based on
Dark Energy Task Force assumptions and having 5\% distance precision
for each of 100 time delay lenses. The Stage IV cosmological probes
being compared in an  open CDM cosmological model with time-variable
dark energy equation of state are weak lensing (WL), BAO, supernovae
(SN), cluster mass function (CL) and time delay cosmography (TD).
Figure reproduced from \citet{C+M09b}.}
\label{fig:fisher}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Extrapolations to N lenses assuming X\% precision per time delay
% distance, forecasts.

% FIGURE: Forecasts for 10,50,100,1000 lenses for various cosmological
% models (w, wa+w0, curvature etc etc). CosmoSIS forecasts (ackn. Dave \&
% Elise, ask them).

While Figure~\ref{fig:fisher} allows different cosmological probes to
be compared (and assessed for competitiveness), it does not show the
value of combining those probes. Indeed, \citet{Lin11} found that the
particular combination of a type Ia supernova dataset with a time  delay
lens dataset holds promise, with a sample of 150 time delay distances,
each measured to 5\% precision, improving the dark energy figure of
merit by a factor of about 5 over what could be  plausibly obtained with
a sample of about 1000 Stage 3 supernovae and a Planck CMB prior alone.

More recently, \citet{JeeKomatsuSuyu2015} have pointed out that
cosmological parameter forecasts for time delay lens samples are
conservative, if each lens is assumed only to measure the time delay
distance. Including the angular diameter distance dependence as well can
have a marked effect on the projection, especially if the spectroscopic
constraints on the lens mass distribution are assumed to be very strong.
The reproduction of Figure 5 from
\citet{JeeEtal2016} in the lefthand panel of Figure~\ref{fig:DdDdt}
illustrates this. These authors find that a future sample of
50 lenses
with 5\% measurements of both time delay distance and angular diameter
distance would increase the figure of merit by a factor of two over that
provided by a Stage~3 supernova and BAO joint analysis. The righthand
panel of Figure~\ref{fig:DdDdt} puts such improvements in the  current
observational context. In the B1608$+$656 analysis, the  angular
diameter distance dependence {\it was} accounted for during the
calculation of the predicted time delay and velocity dispersion data,
but the constraints on the angular diameter distance were not strong:
assigning a uniform prior PDF for the cosmological parameters rather
than the distances introduced degeneracy between $\Dd$ and $\Ddt$,
which then seems to have been broken primarily by the time delay
information to yield a $5.7\%$ precision prediction for $\Ddt$, and
a corresponding $8.1\%$ precision prediction for $\Dd$.
With spatially resolved spectroscopy we should anticipate the angular
diameter distance becoming more important in future analyses, with
some work on simulated data needed to quantify this.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!t]
\begin{minipage}{0.48\linewidth}
    \centering\includegraphics[width=\linewidth]{figures/Jee16_fig5b.pdf}
\end{minipage}\hfill
\begin{minipage}{0.48\linewidth}
    \centering\includegraphics[width=\linewidth]{figures/B1608_DdtDa.pdf}
\end{minipage}
\caption{Cosmological information from angular diameter distances
as well as time delay distances. Left: Fisher matrix
forecasts for a future time delay lens sample where 5\% precision on
each distance is assumed: the combination of distances gives a
significantly more powerful constraint than the time delay distances
would alone \citep[reproduced from][]{JeeEtal2016}. ``$D_{\rm A}(EL)$'' is
the angular diameter distance to the lens from Earth. Right: marginalized
prior (gray) and posterior (blue) PDFs for the two
distances in the B1608$+$656 system, assuming uninformative priors for the
cosmological parameters of a flat $\Lambda$CDM model and offset and
rescaled to reveal the implied percentage precisions of 5.7\% and 8.1\%
in $\Ddt$ and $\Dd$ respectively. }
\label{fig:DdDdt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


What will it take to reach the required level of
distance precision in each of 100 lenses?
If \citet{LiaoEtal2015} are right, samples of over 100 lenses with
precise time delays should be able to be constructed from analysis of
the LSST light curves. The lens models for these systems should be able
to be constrained to sufficient precision using high resolution imaging
from next generation AO facilities, giant segmented mirror telescopes,
JWST and even WFIRST \citep{Men++15}. High overall mass model precision,
and the exploitation of the angular diameter distance dependence,
will require high signal to noise, spatially-resolved spectroscopy as
well: integral field units on
these same telescopes should be able to provide this. The contribution
to the distance uncertainty from the line of sight was particularly high
in the  cases of B1608$+$656 and RXJ1131: in most other systems in a large
sample the effects of the environment should be lesser, and they should
(effectively) average down \citep{Col++13}. The main challenge in
reaching the required Stage IV is likely to be the analysis cost: lens
modeling is something of a craft, and scaling up to large samples could
pose problems. Distributing the work among a large team of modelers will
be needed, an approach that is proving effective in the Frontier Fields
cluster lensing project \citep{FF}.



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{Accuracy [PJM]}
\label{ssec:accuracy}

While the precision available from Stage 3 and Stage 4 samples makes
time delay lenses an interesting prospect for cosmology, they will, like
the other probes, be limited by systematic errors. As the forecasts
show, competitive contributions to joint dark energy parameter
inferences correspond to sub-percent precision in characteristic
distance, which implies that the residual systematic (that is,
post-combination) error in  distance needs to be well below 1\% {\it per
lens}. In this section we revisit the primary sources of systematic
error and  assess the prospects for this stringent requirement to be
met.

\noindent{\bf Time delay measurement:}
\citet{TDC1} showed that a mean accuracy of 0.1\% per lens would already
be achievable in plausible samples of several hundred LSST lenses, were
all images to be taken in the same band.
While these results are encouraging, questions
about our ability to measure time delays from sparse, multi-filter light
curves extracted at scale from realistic images remain
\citep{TCM13}.
Time delay measurement accuracy from  LSST multi-filter
light curve could be tested by a second challenge; the success of the
analyses is likely to hinge on the treatment of AGN color variability
\citep[see e.g.\ ][and references therein]{SunEtal2014}
and chromatic  microelensing
\citep[see e.g.][and references therein]{HainlineEtal2013}.
Joint inference from whole samples is likely to
be important, in mitigating against both outliers and also imprecision
arising from inappropriate uniform priors on  population parameters.
Insights into AGN physics and the stellar composition  of lens galaxies
would be welcome by-products of such an analysis. An alternative approach
could be to continue to puruse single-filter monitoring, but at
increased efficiency. Experiments with higher cadence campaigns,
exploiting the short (sub-day) timescale variability of AGN are in
progress \citep[][F.~Courbin, priv.\ comm.]{BorosonEtal2016}.


\noindent{\bf Lens mass modeling:}
\citet{S+S13} have pointed out the possibility of systematic errors at
the twenty percent level due to modeling assumptions (and their
interaction with the mass sheet degeneracy) when  fitting lensing data
alone.   \citet{Suy++14} fitted the same two models used by
\citet{S+S13}  to the current state of the art Einstein ring imaging and
lens galaxy velocity dispersion data,  and found that present
measurememts of stellar kinematics reduces the error by a factor of 10.
It is not clear  how much of the residual 2\% uncertainty is random: we
don't yet know  how our mass modeling methods respond to the variety of
lens mass  distributions we expect. An important first step has been
taken by  \citet{XuEtal2016}, who looked at the density profiles of a
sample of mock galaxies from the Illustris simulation, finding
significant departures from simple power law profiles. However, they
note that at the mass scales  typical of galaxy scale lenses, and
perhaps with judicious selection of systems whose power law model slope
is close to the isothermal value,  the residual bias in the Hubble
constant could be restricted to a few percent.

As a result of these investigations, we know that 1) the dynamical
information is as important as the lensing data, 2) simple power law
density profiles are very unlikely to enable sub-percent accuracy to be
reached, and 3) we now have simulated galaxies that are {\it
sufficiently realistic and suitably complex} that we can carry out
meaningful tests where the ground truth is  very different from our
assumed analysis models. The acid test will be whether we can recover
the input cosmological parameters  from realistic simulated high
resolution imaging and stellar spectroscopic data made using galaxies
like those in Illustris.

As we look ahead to samples of dozens to hundreds of lenses in the next
decade,  we can also consider the observational capabilities we will
have in that time period. Giant segmented mirror telescopes (including
the James Webb Space Telescope as well as planned ground-based
facilities like TMT, GMT and E-ELT) will bring up to a factor of 10
increase in angular resolution beyond what HST and today's 10-m class
adaptive optics-enabled telescopes can deliver, improving  further the
available Einstein ring constraints. These facilities will  be
instrumented with Integral Field Unit spectrographs, that will  provide
spatially-resolved spectrocopy of the lens galaxy stellar populations.
It is yet to be seen how accurately time delay lenses can be modeled with
such data: extending the realistic lens galaxy data simulation work to include
them would seem to be very important.

The upcoming increase in available precision per lens will support
sigificantly more flexible mass models. While early efforts in this direction
employed regularized grids of pixels \citep{Koo05,SuyuEtal2009,V+K09a},
the most recent attempts model perturbations to simple density profiles with
orthogonal basis sets \citep{BirrerEtal2015}. The opportunity here is to find a
flexible mass model whose parameters can be taken to have been drawn from
a relatively simple prior PDF, which could be derived from either large samples
of observed non-lens galaxies, plausible hydrodynamic simulated galaxies, or both.


The mass sheet degeneracy, and indeed all model parameter degeneracies
are broken by incorporating more information, but this needs to be done
in such a way as to not introduce bias. Using flexible mass models with
reasonably broad but not uninformative  priors is the first step, but
unless these priors are themselves movable, the introduced bias will
remain. The answer is to learn the hyper-parameters that govern the
intrinsic distribution of mass model parameters from the data as well. It is
really the prior on these ``hyper-parameters'' that  needs to come from
simulations. An initial attempts at this kind of ``hierarchical
inference'' can be found in  the analysis of \citet{SonnenfeldEtal2015},
where the authors  infer the values of some 28 hyper-parameters assumed
to govern the  scaling relations between massive galaxies, as well as
the selection function of the lens sample. As surveys yield larger and
larger samples of lenses, we can think of  carrying out joint inferences
with ensembles of both time delay lenses, and also all other lenses, to
bring in more information about the density structure of somewhat
self-similar massive galaxies.


\noindent{\bf Environment and line of sight characterization:}
3) Environment and line of sight. Far off clusters and nearby galaxies
both important (McCully, Collett). Unaccounted for, additional WL
would cause lens mass density profile to be mis-inferred, and
time delays predicted innacurately.
Current methodology ties us to
simulations and reference fields. Wide field surveys will help with the
latter, providing much larger, more homogenous sets of control fields.
Systematic error assiciated with calibrating against simulations is
biggest problem. Hint of magnitude from Collett et al, two Mstar-Mh
relations. NUmber counts methods (Fasnacht, Greene) include ratio, but
this is not  watertight, and needs testing. Light cone reconsutruction
of McCully and Collett provides route to liberation from simulations,
but mroe information about  mass in the unoverse is required. Weak
lensing and clustering can both be used. Halo models constrained by
summary statistics already potentially  useful, althoughg scatter in
relations needs propagating.  COuld learn hyper parameters from all data
but inference is enormous. Covariance with cosmic shear, galaxy
clustering, halo mass function would need to be accounted for -- may
turn out to be negligible (similar to lensing of supernovae?). Mitigation
by selecting low density lines of sight, selection still needs to be done
with care, and propagating all uncertainties.


Other things. Time delay perturbations due to small scale structure in
lens plane or along line of sight, additional source of random noise,
maybe systematic too? Interesting additional probe of CDM substructure.
Needs hour-scale precision. Even when all known systematics have been
investigated and tested, others unforeseen may remain. Strategies for
detecting these include jack-knife tests, possible with large sample.
Other kinds
of ``null tests'' may be possible with a sample of lenses -- research needed
on developing those.
Ultimate test is consistency with other datasets: but for this to be
meaningful the analysis of each dataset must be done blindly, to avoid
unconscious experimenter bias, groupthink towards concordance.  Above
tests need to be done before inblinding.
End to end tests on highly realistic mock data will become ever more
important, to enable full testing while blinded. Time delay challenge
was blind,modeling and environment challenges are needed too. Success
at blind parameter recovery on realistc mock sample is the only way
to assure confidence.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\subsection{Roadmap [TT]}
\label{ssec:roadmap}

% [DO WE WANT TO HAVE SEPARATE SUBSECTION CALLED ROADMAP, OR IS IT GOING
% TO BE EMBEDDED IN THE TWO SUBSECTIONS ABOVE. I THINK IT MAY BE
% EFFECTIVE TO CALL IT OUT]
