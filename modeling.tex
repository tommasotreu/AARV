In addition to time delays, the second main ingredient entering the
determination of time delay distances is the mass model of the main
deflector. In the early days of time delay cosmography one could only
rely on the relative positions of the multiple images as constraints
(since in general the flux ratios are affected by micro and
millilensing, variability, and differential dust extinction, and are
therefore highly uncertain). Even for a quadruply imaged quasars, the
five positional constraints are insufficient to determine Fermat
potential differences to the desired level of precision and accuracy.

There are two classes of solution to the problem of underconstrained
lens models. One is to analyze large samples of lenses with physically
motivated priors and exploit the fact that cosmological parameters are
the same for all lenses to remove model degeneracies. A number of
attempts along these lines have been made \citep{Ogu07b,RK++2015}, and
it is easy to imagine that this solution will be popular in the
future, when large samples of lenses with measured time delays will be
available.

The alternative solution is to increase dramatically the number of
emprical constraints per lens system by means of dedicated high
resolution imaging and spectroscopic observations
\citep{Suy++10,Suy++13,Suy++14}. We describe this approach in detail
below.

For simplicity, in this section we describe only the case of a single
deflector in a single plane, leaving line of sight and environmental
effects for a later section. For clarity, we describe each step
corresponding to a different dataset individually. Ideally, all the
data, including the time delays, should be modeled holistically at the
same time---although in practice the problem has, to date, been broken up
into parts to make it more tractable.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{High resolution imaging observations}

Lensed quasars reside in a host galaxy. For typical redshifts of lens
and source, the host galaxy apparent size is of order
arcseconds. Images with sufficient depth and resolution to isolate the
bright point source and detect the lower surface brightness host
galaxy, often reveal extended lensed features connecting the
point-like images themselves (e.g. Figure~\ref{fig:oldvsmodernimage}).

In the best conditions these images cover hundreds if not thousands of
resolution elements. The distortion of the detailed features of the
lensed images are a direct measurement of the variation of the
deflection angle between the images.  In principle, for data with
infinite signal-to-noise ratio and resolution one could imagine
integrating the gradient of the deflection angle along a path between a
pair of images to obtain the difference in Fermat potential,
up to a mass sheet transformation (Section\ref{sec:theory}).
In practice, in the presence of noisy data and limited resolution,
forward modeling approaches have been the most successful so far, as
discussed below. From an observational point of view, it has been
demonstrated that images with $0.1''-0.2''$ FWHM resolution provide
good results, provided that the point spread function can be
appropriately modeled or reconstructed as part of the lens model
itself. The Hubble Space Telescope in the optical/near infrared
\citep{Suy++10,Suy++13,Suy++14,BirrerEtal2015} and the Very Large Baseline
Interferometer in the radio \citep{WBB04} have been the main sources
of images for this application. Recent progress in adaptive optics
imaging at the 10m W.M.~Keck \citep{Che++16}, the beautiful data being
obtained for lensed source by ALMA \citep{Hez++13a}, and the many
facilities currently being constructed or planned \citep{Men++15},
indicate that the prospects to scale up the number of systems with
available high resolution images are bright.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{Lens modeling techniques}

Conceptually, a detailed model of a lensed quasar and its host galaxy
needs to describe three different physical components: i) the surface
brightness of the source; ii) the surface brightness of the deflector;
iii) the gravitational potential of the deflector. It is useful to
conceptualize the problem in this way, in order to understand where
the information needed to break the degeneracy in the intrepretation
of the data comes from. Lensing is achromatic and preserves surface
brightness so any feature that belongs to the source \cite[including
in line of sight velocity][]{Hez++13} should appear in all the
multiple images (appropriately distorted). Likewise, the deflector is
typically a massive early-type galaxy with smooth surface brightness
distribution and approximately uniform colors \cite[except for dust,
see, e.g.][]{Suy++10}.

Each of the three components is typically described in terms of one or
both of the following choices:
i) simply parametrized functions such as a Sersic profile
for the surface brightness of the lens or the source, and a singular
isothermal ellipsoid for the gravitational potential of the deflector
\citep[e.g.][]{Mar++07,Kne++11,Kee11}; ii) as combinations of basis sets like surface brightness
pixels, lens potential pixel values, or Gauss-Hermite (``shapelet'')
functions
\citep[e.g.][]{Col08, BirrerEtal2015, Nig++15, TagoreAndJackson2016}. The latter class of
very flexible models require regularization to avoid overfitting the
noise in the data.\footnote{In the case of the shapelet basis set,
regularization can effectively be achieved through choosing the number of basis
functions to use as well as the scale of the underlying Gaussian. Most
analyses using shapelets having taken this approach to date, with
\citet{TagoreAndJackson2016} being a notable exception. A
promising alternative scheme would be to assign a less physically-motivated prior for the
shapelet coefficients.}
Hybrid approaches have been proposed where the parametrization of some of
the components is simple and others are complex
\citep{W+D03,T+K04,BrewerAndLewis2006,Suy++06}, or where flexibly-parametrized
``corrections'' are added to simply
parametrized models \citep{Koo05,V+K09,S+H10,Suy++10,BirrerEtal2015}.
The variety of
approaches in the literature reflect the inevitable tensions between
the need to impose as many physically motivated assumptions as
possible, while retaining sufficient flexibility to obtain a realistic
estimate of the uncertainties and avoid introducing biases by
asserting incorrect simplistic models. If the model is too constrained by the
assumption it will lead to underestimated errors, if it is more
flexible than necessary it will lead to a loss of precision.

Once the choice of modeling parametrization is set, exploring the
posterior PDF for the parameters is numerically non-trivial, often requiring weeks to months
of computing time. Fortunately, there are techniques to speed up the
calculations by limiting the number of non-linear parameters. For
example, for a given lens model, the transformation between source and
image plane can be described as a linear operation, or the pixellated
corrections to the potential can be found by linearizing the lens
equation (see references above).

Ideally, modeling choices should be explored systematically as well,
since they can potentially introduce systematic errors. This is
currently being done in the most advanced studies, at great expense in
term of computing time and investigator time. As we discuss in
Section~\ref{sec:outlook}, speeding up the modeling phase and reducing
the investigator time per system will be key to analyzing the large
statistical samples expected in the future.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{The role of stellar kinematics}

As introduced in Section~\ref{sec:theory}, stellar kinematics provide
a qualitatively different input and are therefore very valuable in
breaking degeneracies in the intepretation of lensing data
\citep[e.g., the mass-sheet degeneracy][]{Koo++03}, and in estimating systematic
uncertainties. Of course, translating kinematic data into estimates of
gravitational potential has its own uncertainties and degeneracies
(e.g. the mass anisotropy degeneracy for pressure supported systems),
but the combination of the two datasets in the context of a single
mass model has been proven to be very effective
\citep{T+K02a,T+K04}. Even a single measurement of stellar velocity dispersion,
interpreted via simple spherical Jeans modeling has been shown to
substantially reduce modeling uncertainties
\citep{T+K02b,Koo++03,Suy++14}. It is clear that getting spatially
resolved kinematic data will enable breaking the mass-anistropy
degeneracy \citep[see, e.g.,][and references therein]{Cou++14} and
thus better constraints on the lens model and thus of the resulting
cosmological inference (Agnello et al. 2016, in prep).
