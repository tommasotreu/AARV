Once the time delays have been measured, the second main ingredient
entering the determination of time delay distances is the mass model
of the main deflector. In the early days of time delay cosmography one
could only rely on the relative positions of the multiple images as
constraints (since in general the flux ratios are affected by micro
and millilensing, variability, and differential dust extinction, and
are therefore highly uncertain). Even for a quadruply imaged quasars,
the six positional constraints are insufficient to determine Fermat
potential differences to the desired level of precision and accuracy.

There are two classes of solution to the problem of underconstrained
lens models. One is to analyze large samples of lenses with physically
motivated priors and exploit the fact that cosmological parameters are
the same for all lenses to remove model degeneracies. A number of
attempts along these lines have been made in the past \citep{Ogu07},
and it is easy to imagine that this solution will be popular in the
future, when large samples of lenses with measured time delays will be
available.

The alternative solution is to increase dramatically the number of
emprical constraints per lens system by means of dedicated high
resolution imaging and spectroscopic observations
\citep{Suy++10,Suy++13,Suy++14}. We describe this approach in detail
below.

For simplicity, in this section we describe only the case of a single
deflector in a single plane, leaving line of sight and environmental
effects for a later section. For clarity, we describe each step
corrisponding to a different dataset individually. Ideally all the
data, including the time delays, should be modeled holistically at the
same time, although in practice so far the problem has been broken up
in parts to make it more tractable.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{High Resolution Imaging Observations}

Lensed quasars reside in a host galaxy. For typical redshifts of lens
and source, the host galaxy apparent size is of order
arcseconds. Images with sufficient depth and resolution to isolate the
bright point source and detect the lower surface brightness host
galaxy, often reveal extended lensed features connecting the
point-like images themselves (e.g. Figure~\ref{fig:oldvsmodernimage}).

In the best conditions these images cover hundreds if not thousand
resolution elements. The distortion of the detailed features of the
lensed images are a direct measurement of the variation of the
deflection angle between the images.  In principle, for data with
infinite signal-to-noise ratio and resolution one could imagine
integrating the gradient of the deflection angle along a path betwen a
pair of images to obtain the difference in Fermat potential. In
practice, in the presence of noisy data and limited resolution,
forward modeling approaches have been the most successful so far, as
discussed below. From an observational point of view, it has been
demonstrated that images with $0.1''-0.2''$ FWHM resolution provide
good results, provided that the point spread function can be
appropriately modeled or reconstructed as part of the lens model
itself. The Hubble Space Telescope in the optical/near infrared
\citep{Suy++10,Suy++13,Suy++14,BirrerEtal2015} and the Very Large Baseline
Interferometer in the radio \citep{WBB04} have been the main sources
of images for this application. Recent progress in adaptive optics
imaging at the 10m W.M.~Keck \citep{Che++16}, the beautiful data being
obtained for lensed source by ALMA \citep{Hezaveh++13}, and the many
facilities currently being constructed or planned \citep{Men++15},
indicate that the prospects to scale up the number of systems with
available high resolution images are bright.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{Lens Modeling Techniques}

Conceptually, a detailed model of a lensed quasar and its host galaxy
needs to describe three different physical components: i) the surface
brightness of the source; ii) the surface brightness of the deflector;
iii) the gravitational potential of the deflector. It is useful to
conceptualize the problem in this way, in order to understand where
the information needed to break the degeneracy in the intrepretation
of the data comes from. Lensing is achromatic and preserves surface
brightness so any feature that belongs to the source \cite[including
in line of sight velocity][]{Hez++13} should appear in all the
multiple images (appropriately distorted). Likewise, the deflector is
typically a massive early-type galaxy with smooth surface brightness
distribution and approximately uniform colors \cite[except for dust,
see, e.g.][]{Suy++10}.

Each of the three components is typically described in terms of one or
two choices: i) simply parametrized functions such as a Sersic profile
for the surface brightness of the lens or the source, and a singular
isothermal ellipsoid for the gravitational potential of the deflector
\citep{Kee11}; ii) as combination of basis sets like surface brightness
pixels, potential pixels, or gauss-hermite functions
\citep[e.g.][]{Col10, BirrerEtal2015, Nig++15}. These so-called
``pixellated'' models require regularization to avoid overfitting
noise in the data. Hybrid approaches have been proposed where some of
the components are simply parametrized and others are pixellated
\citep{W+D03,T+K04,Suy++06}, or where pixels are used as corrections to simply
parametrized models \citep{Koo05,V+K09,S+H10,Suy++10}. The variety of
approaches in the literature reflect the inevitable tensions between
the need to impose as many physically motivated assumptions as
possible while retaining sufficient flexibility to obtain a realistic
estimate of the uncertainties. If the model is too constrained by the
assumption it will lead to underestimated errors, if it is more
flexible than necessary it will lead to a loss of information.

Once the choice of modeling parametrization is set, exploring the
likelihood is numerically non-trivial, often requiring weeks to months
of computing time. Fortunately, there are techniques to speed up the
calculations by limiting the number of non-linear parameters. For
example, for a given lens model, the transformation between source and
image plane can be described as a linear operation, or the pixellated
corrections to the potential can be found by linearizing the lens
equation (see references above). 

Ideally, modeling choices should be explored systematically as well,
since they can potentially introduce systematic errors. This is
currently being done in the most advanced studies, at great expense in
term of computing time and investigator time. As we discuss in
Section~\ref{sec:outlook}, speeding up the modeling phase and reducing
the investigator time per system will be key to analyzing the large
statistical samples expected in the future.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{The Role of Stellar kinematics}

As introduced in Section~\ref{sec:theory}, stellar kinematics provide
a qualitatively different input and therefore very valuable in
breaking degeneracies in the intepretation of lensing data (e.g. the
mass-sheet degeneracy), and in estimating systematic uncertainties. Of
course, translating kinematic data into estimates of gravitational
potential has its own uncertainties and degeneracies (e.g. the mass
anisotropy degeneracy for pressure supported systems), but the
combination of the two datasets in the context of a single mass model
has been proven to be very effective
\citep{T+K04}. Even a single measurement of stellar velocity dispersion,
interpreted via
simple spherical Jeans modeling has been shown to substantially
reduce modeling uncertainties \citep{T+K02b,Koo++03,Suy++14}. It is
clear that getting spatially resolved kinematic data will allow for
better constraints on the lens model and thus of the resulting
cosmological inference.

[PJM: I THINK THIS SECTION IS THE PLACE TO DESCRIBE THE PARTICULAR
CHOICES THAT HAVE BEEN MADE FOR JOINT ANALYSIS OF LENSING AND KINEMATICS
DATA --  THE THEORY SECTION IS BETTER LEFT MORE GENERAL. WE COULD SKETCH
THE SUYU ET AL 2013 MODEL AND THEN CRITIQUE IT, AND ALSO NOTE ANY
ALTERNATIVE ASSUMPTIONS MADE BY OTHERS (BUT I DON'T THINK THEY WILL BE
VERY DIFFERENT).]
