The measurement of gravitational time delays involves two steps:  taking
monitoring observations of the system over a period of several years,
and then inferring the time delays between the multiple images from
these data.

%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{Monitoring Observations and Results}

Active Galactic Nuclei (AGN) show intrinsic time variability on many
scales, with the variability amplitude increasing with timescale. Long
monitoring campaigns can build up high statistical significance as
more and more light curve features can be brought into play.  However,
such long campaigns are difficult to carry out in practice, because a
large number of guaranteed observing nights are required (even if the
total exposure time is modest). Scheduling such a program has proved
difficult in traditional time allocation schemes, due to the competing
demands of the rest of the astronomy community and the long duration
requirements of lens monitoring. The highest precision time delays
have come from monitoring campaigns carried out with dedicated
facilities so far, i.e. observatories that were either able to commit
to the long term monitoring proposal submitted, or that were actually
operated in part by the monitoring collaboration.


Monitoring of the CLASS lens B1608$+$656 in the radio with the Very
Large Array enabled the breakthrough  time delay measurements of
\citep{Fas++02}. In its first season, this program  yielded measurements
of all three time delays in this quadruple image system with precision
of 6--10\% \citep{Fas++99}; with the variability of the source
increasing over the subsequent two seasons, \citep{Fas++02} were able
to reduce this uncertainty to 2--5\%. Such high precision was the
result of a dedicated campaign which consisted of 8-month seasons,
with a mean observation spacing of around 3 days. The light curves
were calibrated to 0.6\% accuracy.

While time delays had previously been measured in ten other lens
systems, this was the first time that all the delays in a quad had
been obtained; moreover, it brought the time delay uncertainty below
the systematic uncertainty due to the lens model, prompting new
efforts in this direction beyond what \citet{K+F99} needed to do.

While B1608$+$656 is not the only radio lens with measured time delays,
a combination of factors  led the observational focus to shift towards
monitoring in the optical. With the sample of known, bright lensed
quasars increasing in size,  networks of 1-2m class optical telescopes
began to be investigated. The variability in these systems is somewhat
more reliable, and while microlensing  and image resolution present
observational challenges, the access to data was found to be less
restrictive. The COSMOGRAIL project took on the task of measuring lens
time delays with few-percent precision in this way:  \citet{Eig++05}
showed that microlensing was likely not to be an insurmountable task,
and \citet{Vui++05} provided the proof of  concept with a 4\% precision
time delay measurement in SDSS\ J1650$+$4251.

One of the keys to the success of this program has been the simultaneous
deconvolution of the individual frames in the imaging dataset, using  a
mixture model to describe the point-like quasar images and extended lens
and AGN host galaxies \citep{MCS98}.  Another is the dedicated nature of
the network of telescopes employed, and the  careful calibration of the
photometry across this distributed system. Seasons of 8--12 months
duration over campaigns of up to 9 years have been achieved, with
typical mean observation gaps of around 3--4 days.

The COSMOGRAIL team have now published high precision time delays in
WFI\,J2033$-$4723 \citep[][3.8\%]{Vui++08}, HE\,0435$-$1223
\citep[][5.6\%]{Cou++11}, SDSS\,J1206$+$4332 \citep[][2.7\%]{Eul++13}
and  RX\,J1131$-$1231 \citep[][1.5\%]{Tew++13}, and SDSS\,J1001$+$5027
\citep[][2.8\%]{RK++13}, with more due to follow.  Typically multiple
years of monitoring is needed to obtain an accurate  time delay, as the
variability fluctuates and the reliability of the  measurement converges
\citep[see the discussion in e.g.\ ][]{Tew++13}.

A consistent picture seems to emerge from both the VLA and COSMOGRAIL
monitoring projects: high precision gravitational time delay
measurement requires campaigns consisting of multiple, long seasons,
with around 3-day cadence. The baseline observing strategy for the
Large Synoptic Survey Telescope (LSST) is somewhat different to this,
with seasons expected to be around 4--5 months in length, and gaps
between observation nights only reaching 4--5 days when images in all
filters are taken into account. The ``Time Delay Challenge'' project
was designed to test the measurability of lens time delays with such
light curves \citep{DoblerEtal2013}, in a blind test offered to the
astronomical community. From the ten algorithms entered by seven
teams, it was concluded that time delay estimates of the precision and
accuracy needed for time delay cosmography would indeed be possible,
in $\sim$400 LSST lensed quasar systems \citep{LiaoEtal2015}. This
result came with two caveats: 1) the single filter light curve data
presented in the challenge is representative of the multi-filter data
we actually expect, and 2) that ``outliers'' (catastrophic time delay
mis-estimates) will be able to be caught during the measurement
process.  A second challenge to test these assumptions is in
preparation.



%   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %

\subsubsection{Lightcurve Analysis Methods}

How were the time delays surveyed in the previous section derived from
the light curve data? Interest in this particular inference problem
has been high since the controversies of the late
1990's. \citet{Fas++99} used the ``dispersion method'' of
\citet{Pelt++96}, a technique that involves shifting one observed
light curve relative to another (both in time and in amplitude) and
minimizing the dispersion between adjacent points in the resulting
composite curve. Uncertainties were estimated by Monte Carlo
resampling of the data, assuming the minimum dispersion time delay and
magnification ratio to be true. In order to take into account the
slowly varying incoherent microlensing signals present in their
optical light curve data, the COSMOGRAIL team have investigated three
analysis techniques that all involve interpolation of the light curves
in some way \citep{TCM13}: free-knot splines, Gaussian processes and
simple linear interpolation have all been tested, within a common
``python curve-shifting'' (PyCS) framework.\footnote{The COSMOGRAIL
curve shifting analysis code is available from
\texttt{http://cosmograil.org}} These agree with each other given
light curves of sufficient length, providing an argument for
multiple-season monitoring campaigns.

The time delay challenge prompted seven analysis teams to develop and
test algorithms for time delay estimation. These are outlined in the
TDC1 analysis paper of \citet{LiaoEtal2015}, but we give a very brief
summary here as well, along with updated references. The PyCS team tried
a three-step approach (visual inspection and interactive curve shifting,
followed by spline fitting, followed  by an additional spline model
regression analysis of the residuals), and submitted an entry after each
step \citep{BonvinEtal2016}. Two other teams applied similar
curve-shifting approaches: both \citet{A+S2015} and \citet{RK++2015}
devised smoothing and cross-correlation schemes that they find to be
both fast and reliable. Jackson applied the dispersion method of
\citet{Pelt++96}, but carefully supervised via visual inspection to
check for  catastrophic failures.  The three remaining teams used
Gaussian Processes (GPs) to model the light curves. \citet{TakEtal2016}
used a custom Gibbs sampler to infer the hyper-parameters describing the
GP for the AGN variability and polynomials for the microlensing signals,
although they ignored microlensing during the challenge itself.
Romero-Wolf \& Moustakas implemented a very similar model, also ignored
microlensing, and used a freely-available ensemble sampler for the
inference. \citet{H+L2014} used GPs for both the AGN and microlensing
variability, and optimized the hyper-parameters rather than sampling
them.

Two factors were important in the minimisation of catastrophic time
delay mis-estimation: explicitly including microlensing in the model,
and  visual inspection of the results. An additional promising avenue
for future challenges ought to be ensemble analysis, to exploit 1) the
intrinsic correlations between, for example, AGN variability, color and
brightness, and 2) the fact that  the cosmological parameters are common
to all lens systems.
